{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "node_num, feat_dim, stat_dim, num_class, T\n",
    "feat_Matrix, X_Node, X_Neis, dg_list\n",
    "'''\n",
    "content_path = \"./cora/cora.content\"\n",
    "cite_path = \"./cora/cora.cites\"\n",
    "\n",
    "# 读取文本内容\n",
    "with open(content_path, \"r\") as fp:\n",
    "    contents = fp.readlines()\n",
    "with open(cite_path, \"r\") as fp:\n",
    "    cites = fp.readlines()\n",
    "\n",
    "contents = np.array([np.array(l.strip().split(\"\\t\")) for l in contents])\n",
    "paper_list, feat_list, label_list = np.split(contents, [1,-1], axis=1)\n",
    "paper_list, label_list = np.squeeze(paper_list), np.squeeze(label_list)\n",
    "# Paper -> Index dict\n",
    "paper_dict = dict([(key, val) for val, key in enumerate(paper_list)])\n",
    "# Label -> Index 字典\n",
    "labels = list(set(label_list))\n",
    "label_dict = dict([(key, val) for val, key in enumerate(labels)])\n",
    "# Edge_index\n",
    "cites = [i.strip().split(\"\\t\") for i in cites]\n",
    "cites = np.array([[paper_dict[i[0]], paper_dict[i[1]]] for i in cites], \n",
    "                 np.int64).T   # (2, edge)\n",
    "cites = np.concatenate((cites, cites[::-1, :]), axis=1)  # (2, 2*edge) or (2, E)\n",
    "# Degree\n",
    "_, degree_list = np.unique(cites[0,:], return_counts=True)\n",
    "\n",
    "# Input\n",
    "node_num = len(paper_list)\n",
    "feat_dim = feat_list.shape[1]\n",
    "stat_dim = 32\n",
    "num_class = len(labels)\n",
    "T = 2\n",
    "feat_Matrix = torch.Tensor(feat_list.astype(np.float32))\n",
    "X_Node, X_Neis = np.split(cites, 2, axis=0)\n",
    "X_Node, X_Neis = torch.from_numpy(np.squeeze(X_Node)), \\\n",
    "                 torch.from_numpy(np.squeeze(X_Neis))\n",
    "dg_list = degree_list[X_Node]\n",
    "label_list = np.array([label_dict[i] for i in label_list])\n",
    "label_list = torch.from_numpy(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Data Process Info********************\n",
      "==> Number of node : 2708\n",
      "==> Number of edges : 10858/2=5429\n",
      "==> Number of classes : 7\n",
      "==> Dimension of node features : 1433\n",
      "==> Dimension of node state : 32\n",
      "==> T : 2\n",
      "==> Shape of feat_Matrix : torch.Size([2708, 1433])\n",
      "==> Shape of X_Node : torch.Size([10858])\n",
      "==> Shape of X_Neis : torch.Size([10858])\n",
      "==> Length of dg_list : 10858\n"
     ]
    }
   ],
   "source": [
    "print(\"{}Data Process Info{}\".format(\"*\"*20, \"*\"*20))\n",
    "print(\"==> Number of node : {}\".format(node_num))\n",
    "print(\"==> Number of edges : {}/2={}\".format(cites.shape[1], int(cites.shape[1]/2)))\n",
    "print(\"==> Number of classes : {}\".format(num_class))\n",
    "print(\"==> Dimension of node features : {}\".format(feat_dim))\n",
    "print(\"==> Dimension of node state : {}\".format(stat_dim))\n",
    "print(\"==> T : {}\".format(T))\n",
    "print(\"==> Shape of feat_Matrix : {}\".format(feat_Matrix.shape))\n",
    "print(\"==> Shape of X_Node : {}\".format(X_Node.shape))\n",
    "print(\"==> Shape of X_Neis : {}\".format(X_Neis.shape))\n",
    "print(\"==> Length of dg_list : {}\".format(len(dg_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xi(nn.Module):\n",
    "    def __init__(self, ln, s):\n",
    "        super(Xi, self).__init__()\n",
    "        self.ln = ln   # 节点特征向量的维度\n",
    "        self.s = s     # 节点的个数\n",
    "        \n",
    "        # 线性网络层\n",
    "        self.linear = nn.Linear(in_features=2 * ln,\n",
    "                                out_features=s ** 2,\n",
    "                                bias=True)\n",
    "        # 激活函数\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        bs = X.size()[0]\n",
    "        out = self.linear(X)\n",
    "        out = self.tanh(out)\n",
    "        return out.view(bs, self.s, self.s)\n",
    "\n",
    "class Rou(nn.Module):\n",
    "    def __init__(self, ln, s):\n",
    "        super(Rou, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=ln,\n",
    "                                out_features=s,\n",
    "                                bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, X):\n",
    "        return self.tanh(self.linear(X))\n",
    "\n",
    "class Hw(nn.Module):\n",
    "    def __init__(self, ln, s, mu=0.9):\n",
    "        super(Hw, self).__init__()\n",
    "        self.ln = ln\n",
    "        self.s = s\n",
    "        self.mu = mu\n",
    "        \n",
    "        # 初始化网络层\n",
    "        self.Xi = Xi(ln, s)\n",
    "        self.Rou = Rou(ln, s)\n",
    "    \n",
    "    def forward(self, X, H, dg_list):\n",
    "        if isinstance(dg_list, list) or isinstance(dg_list, np.ndarray):\n",
    "            dg_list = torch.Tensor(dg_list).to(X.device)\n",
    "        elif isinstance(dg_list, torch.Tensor):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"==> dg_list should be list or tensor, not {}\".format(type(dg_list)))\n",
    "        A = (self.Xi(X) * self.mu / self.s) / dg_list.view(-1, 1, 1)# (N, S, S)\n",
    "        b = self.Rou(torch.chunk(X, chunks=2, dim=1)[0])# (N, S)\n",
    "        out = torch.squeeze(torch.matmul(A, torch.unsqueeze(H, 2)),-1) + b  # (N, s, s) * (N, s) + (N, s)\n",
    "        return out    # (N, s)\n",
    "\n",
    "class AggrSum(nn.Module):\n",
    "    def __init__(self, node_num):\n",
    "        super(AggrSum, self).__init__()\n",
    "        self.V = node_num\n",
    "    \n",
    "    def forward(self, H, X_node):\n",
    "        # H : (N, s) -> (V, s)\n",
    "        # X_node : (N, )\n",
    "        mask = torch.stack([X_node] * self.V, 0)\n",
    "        mask = mask.float() - torch.unsqueeze(torch.range(0,self.V-1).float(), 1)\n",
    "        mask = (mask == 0).float()\n",
    "        # (V, N) * (N, s) -> (V, s)\n",
    "        return torch.mm(mask, H)\n",
    "\n",
    "\n",
    "class OriLinearGNN(nn.Module):\n",
    "    def __init__(self, node_num, feat_dim, stat_dim, num_class, T):\n",
    "        super(OriLinearGNN, self).__init__()\n",
    "        self.embed_dim = feat_dim\n",
    "        self.stat_dim = stat_dim\n",
    "        self.T = T\n",
    "        # 输出层\n",
    "        '''\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(stat_dim, 16),   # ln+s -> hidden_layer\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(16, num_class)   # hidden_layer -> logits\n",
    "        )\n",
    "        '''\n",
    "        self.out_layer = nn.Linear(stat_dim, num_class)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        # 实现Fw\n",
    "        self.Hw = Hw(feat_dim, stat_dim)\n",
    "        # 实现H的分组求和\n",
    "        self.Aggr = AggrSum(node_num)\n",
    "        \n",
    "    def forward(self, feat_Matrix, X_Node, X_Neis, dg_list):\n",
    "        node_embeds = torch.index_select(input=feat_Matrix,\n",
    "                                         dim=0,\n",
    "                                         index=X_Node)  # (N, ln)\n",
    "        neis_embeds = torch.index_select(input=feat_Matrix,\n",
    "                                         dim=0,\n",
    "                                         index=X_Neis)  # (N, ln)\n",
    "        X = torch.cat((node_embeds, neis_embeds), 1)    # (N, 2 * ln)\n",
    "        H = torch.zeros((feat_Matrix.shape[0], self.stat_dim), dtype=torch.float32)  # (V, s)\n",
    "        H = H.to(feat_Matrix.device)\n",
    "        # 循环T次计算\n",
    "        for t in range(self.T):\n",
    "            # (V, s) -> (N, s)\n",
    "            H = torch.index_select(H, 0, X_Neis)\n",
    "            # (N, s) -> (N, s)\n",
    "            H = self.Hw(X, H, dg_list)\n",
    "            # (N, s) -> (V, s)\n",
    "            H = self.Aggr(H, X_Node)\n",
    "            # print(H[1])\n",
    "        # out = torch.cat((feat_Matrix, H), 1)   # (V, ln+s)\n",
    "        out = self.log_softmax(self.dropout(self.out_layer(H)))\n",
    "        return out  # (V, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivyone/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:116: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 2.0693, train acc 0.1142\n",
      "[Epoch 1/200] Loss 1.7313, train acc 0.3800\n",
      "[Epoch 2/200] Loss 1.4347, train acc 0.4918\n",
      "[Epoch 3/200] Loss 1.2367, train acc 0.5281\n",
      "[Epoch 4/200] Loss 1.1052, train acc 0.5744\n",
      "[Epoch 5/200] Loss 1.0255, train acc 0.5790\n",
      "[Epoch 6/200] Loss 0.9700, train acc 0.6054\n",
      "[Epoch 7/200] Loss 0.9487, train acc 0.6001\n",
      "[Epoch 8/200] Loss 0.8957, train acc 0.6230\n",
      "[Epoch 9/200] Loss 0.8920, train acc 0.6019\n",
      "Accuracy: 0.7600\n",
      "[Epoch 10/200] Loss 0.8451, train acc 0.6288\n",
      "[Epoch 11/200] Loss 0.8192, train acc 0.6352\n",
      "[Epoch 12/200] Loss 0.7872, train acc 0.6499\n",
      "[Epoch 13/200] Loss 0.7985, train acc 0.6393\n",
      "[Epoch 14/200] Loss 0.8149, train acc 0.6335\n",
      "[Epoch 15/200] Loss 0.8032, train acc 0.6370\n",
      "[Epoch 16/200] Loss 0.7923, train acc 0.6376\n",
      "[Epoch 17/200] Loss 0.7545, train acc 0.6581\n",
      "[Epoch 18/200] Loss 0.7832, train acc 0.6423\n",
      "[Epoch 19/200] Loss 0.7590, train acc 0.6429\n",
      "Accuracy: 0.7640\n",
      "[Epoch 20/200] Loss 0.7525, train acc 0.6376\n",
      "[Epoch 21/200] Loss 0.7474, train acc 0.6470\n",
      "[Epoch 22/200] Loss 0.7308, train acc 0.6557\n",
      "[Epoch 23/200] Loss 0.7056, train acc 0.6686\n",
      "[Epoch 24/200] Loss 0.7124, train acc 0.6540\n",
      "[Epoch 25/200] Loss 0.7321, train acc 0.6411\n",
      "[Epoch 26/200] Loss 0.7006, train acc 0.6786\n",
      "[Epoch 27/200] Loss 0.7251, train acc 0.6552\n",
      "[Epoch 28/200] Loss 0.7262, train acc 0.6610\n",
      "[Epoch 29/200] Loss 0.7037, train acc 0.6528\n",
      "Accuracy: 0.7700\n",
      "[Epoch 30/200] Loss 0.6851, train acc 0.6692\n",
      "[Epoch 31/200] Loss 0.7295, train acc 0.6388\n",
      "[Epoch 32/200] Loss 0.6999, train acc 0.6563\n",
      "[Epoch 33/200] Loss 0.7378, train acc 0.6434\n",
      "[Epoch 34/200] Loss 0.7064, train acc 0.6487\n",
      "[Epoch 35/200] Loss 0.7138, train acc 0.6616\n",
      "[Epoch 36/200] Loss 0.7169, train acc 0.6552\n",
      "[Epoch 37/200] Loss 0.7329, train acc 0.6423\n",
      "[Epoch 38/200] Loss 0.7474, train acc 0.6317\n",
      "[Epoch 39/200] Loss 0.7207, train acc 0.6540\n",
      "Accuracy: 0.7780\n",
      "[Epoch 40/200] Loss 0.7316, train acc 0.6446\n",
      "[Epoch 41/200] Loss 0.7067, train acc 0.6569\n",
      "[Epoch 42/200] Loss 0.7071, train acc 0.6598\n",
      "[Epoch 43/200] Loss 0.6759, train acc 0.6780\n",
      "[Epoch 44/200] Loss 0.6843, train acc 0.6663\n",
      "[Epoch 45/200] Loss 0.7052, train acc 0.6546\n",
      "[Epoch 46/200] Loss 0.7087, train acc 0.6710\n",
      "[Epoch 47/200] Loss 0.7262, train acc 0.6417\n",
      "[Epoch 48/200] Loss 0.6796, train acc 0.6633\n",
      "[Epoch 49/200] Loss 0.7066, train acc 0.6499\n",
      "Accuracy: 0.7600\n",
      "[Epoch 50/200] Loss 0.7060, train acc 0.6593\n",
      "[Epoch 51/200] Loss 0.7116, train acc 0.6610\n",
      "[Epoch 52/200] Loss 0.7283, train acc 0.6452\n",
      "[Epoch 53/200] Loss 0.7041, train acc 0.6540\n",
      "[Epoch 54/200] Loss 0.7432, train acc 0.6405\n",
      "[Epoch 55/200] Loss 0.6868, train acc 0.6698\n",
      "[Epoch 56/200] Loss 0.7322, train acc 0.6487\n",
      "[Epoch 57/200] Loss 0.7119, train acc 0.6446\n",
      "[Epoch 58/200] Loss 0.7027, train acc 0.6645\n",
      "[Epoch 59/200] Loss 0.6816, train acc 0.6680\n",
      "Accuracy: 0.7540\n",
      "[Epoch 60/200] Loss 0.6958, train acc 0.6598\n",
      "[Epoch 61/200] Loss 0.7460, train acc 0.6329\n",
      "[Epoch 62/200] Loss 0.6922, train acc 0.6680\n",
      "[Epoch 63/200] Loss 0.6948, train acc 0.6610\n",
      "[Epoch 64/200] Loss 0.7291, train acc 0.6358\n",
      "[Epoch 65/200] Loss 0.7372, train acc 0.6434\n",
      "[Epoch 66/200] Loss 0.7085, train acc 0.6487\n",
      "[Epoch 67/200] Loss 0.7206, train acc 0.6475\n",
      "[Epoch 68/200] Loss 0.7233, train acc 0.6499\n",
      "[Epoch 69/200] Loss 0.6800, train acc 0.6821\n",
      "Accuracy: 0.7580\n",
      "[Epoch 70/200] Loss 0.7179, train acc 0.6487\n",
      "[Epoch 71/200] Loss 0.7017, train acc 0.6452\n",
      "[Epoch 72/200] Loss 0.7122, train acc 0.6575\n",
      "[Epoch 73/200] Loss 0.7039, train acc 0.6704\n",
      "[Epoch 74/200] Loss 0.7004, train acc 0.6581\n",
      "[Epoch 75/200] Loss 0.6922, train acc 0.6686\n",
      "[Epoch 76/200] Loss 0.7189, train acc 0.6370\n",
      "[Epoch 77/200] Loss 0.7275, train acc 0.6587\n",
      "[Epoch 78/200] Loss 0.6887, train acc 0.6651\n",
      "[Epoch 79/200] Loss 0.6955, train acc 0.6487\n",
      "Accuracy: 0.7580\n",
      "[Epoch 80/200] Loss 0.7118, train acc 0.6470\n",
      "[Epoch 81/200] Loss 0.7171, train acc 0.6622\n",
      "[Epoch 82/200] Loss 0.7362, train acc 0.6230\n",
      "[Epoch 83/200] Loss 0.7095, train acc 0.6446\n",
      "[Epoch 84/200] Loss 0.7015, train acc 0.6657\n",
      "[Epoch 85/200] Loss 0.7001, train acc 0.6552\n",
      "[Epoch 86/200] Loss 0.6920, train acc 0.6727\n",
      "[Epoch 87/200] Loss 0.7229, train acc 0.6446\n",
      "[Epoch 88/200] Loss 0.7138, train acc 0.6616\n",
      "[Epoch 89/200] Loss 0.7156, train acc 0.6593\n",
      "Accuracy: 0.7580\n",
      "[Epoch 90/200] Loss 0.7189, train acc 0.6540\n",
      "[Epoch 91/200] Loss 0.7170, train acc 0.6516\n",
      "[Epoch 92/200] Loss 0.7179, train acc 0.6393\n",
      "[Epoch 93/200] Loss 0.7031, train acc 0.6393\n",
      "[Epoch 94/200] Loss 0.7114, train acc 0.6511\n",
      "[Epoch 95/200] Loss 0.6897, train acc 0.6528\n",
      "[Epoch 96/200] Loss 0.6935, train acc 0.6534\n",
      "[Epoch 97/200] Loss 0.6589, train acc 0.6674\n",
      "[Epoch 98/200] Loss 0.7117, train acc 0.6610\n",
      "[Epoch 99/200] Loss 0.7139, train acc 0.6528\n",
      "Accuracy: 0.7560\n",
      "[Epoch 100/200] Loss 0.6800, train acc 0.6674\n",
      "[Epoch 101/200] Loss 0.7010, train acc 0.6587\n",
      "[Epoch 102/200] Loss 0.6847, train acc 0.6587\n",
      "[Epoch 103/200] Loss 0.6981, train acc 0.6581\n",
      "[Epoch 104/200] Loss 0.6887, train acc 0.6593\n",
      "[Epoch 105/200] Loss 0.6733, train acc 0.6780\n",
      "[Epoch 106/200] Loss 0.7326, train acc 0.6352\n",
      "[Epoch 107/200] Loss 0.7234, train acc 0.6516\n",
      "[Epoch 108/200] Loss 0.6947, train acc 0.6446\n",
      "[Epoch 109/200] Loss 0.6759, train acc 0.6604\n",
      "Accuracy: 0.7540\n",
      "[Epoch 110/200] Loss 0.6831, train acc 0.6727\n",
      "[Epoch 111/200] Loss 0.6796, train acc 0.6663\n",
      "[Epoch 112/200] Loss 0.7091, train acc 0.6563\n",
      "[Epoch 113/200] Loss 0.7012, train acc 0.6540\n",
      "[Epoch 114/200] Loss 0.6723, train acc 0.6639\n",
      "[Epoch 115/200] Loss 0.6703, train acc 0.6663\n",
      "[Epoch 116/200] Loss 0.7024, train acc 0.6540\n",
      "[Epoch 117/200] Loss 0.6759, train acc 0.6610\n",
      "[Epoch 118/200] Loss 0.6800, train acc 0.6686\n",
      "[Epoch 119/200] Loss 0.6788, train acc 0.6657\n",
      "Accuracy: 0.7440\n",
      "[Epoch 120/200] Loss 0.7031, train acc 0.6440\n",
      "[Epoch 121/200] Loss 0.6977, train acc 0.6569\n",
      "[Epoch 122/200] Loss 0.7209, train acc 0.6516\n",
      "[Epoch 123/200] Loss 0.6798, train acc 0.6581\n",
      "[Epoch 124/200] Loss 0.7119, train acc 0.6464\n",
      "[Epoch 125/200] Loss 0.6972, train acc 0.6569\n",
      "[Epoch 126/200] Loss 0.6862, train acc 0.6651\n",
      "[Epoch 127/200] Loss 0.7014, train acc 0.6493\n",
      "[Epoch 128/200] Loss 0.6926, train acc 0.6593\n",
      "[Epoch 129/200] Loss 0.7279, train acc 0.6434\n",
      "Accuracy: 0.7520\n",
      "[Epoch 130/200] Loss 0.6934, train acc 0.6575\n",
      "[Epoch 131/200] Loss 0.6770, train acc 0.6727\n",
      "[Epoch 132/200] Loss 0.6763, train acc 0.6780\n",
      "[Epoch 133/200] Loss 0.6910, train acc 0.6639\n",
      "[Epoch 134/200] Loss 0.7233, train acc 0.6370\n",
      "[Epoch 135/200] Loss 0.7144, train acc 0.6528\n",
      "[Epoch 136/200] Loss 0.7108, train acc 0.6581\n",
      "[Epoch 137/200] Loss 0.6873, train acc 0.6657\n",
      "[Epoch 138/200] Loss 0.6961, train acc 0.6628\n",
      "[Epoch 139/200] Loss 0.7254, train acc 0.6329\n",
      "Accuracy: 0.7480\n",
      "[Epoch 140/200] Loss 0.6862, train acc 0.6516\n",
      "[Epoch 141/200] Loss 0.6919, train acc 0.6604\n",
      "[Epoch 142/200] Loss 0.7029, train acc 0.6446\n",
      "[Epoch 143/200] Loss 0.6351, train acc 0.6745\n",
      "[Epoch 144/200] Loss 0.7285, train acc 0.6552\n",
      "[Epoch 145/200] Loss 0.6872, train acc 0.6803\n",
      "[Epoch 146/200] Loss 0.6563, train acc 0.6739\n",
      "[Epoch 147/200] Loss 0.7079, train acc 0.6382\n",
      "[Epoch 148/200] Loss 0.6854, train acc 0.6745\n",
      "[Epoch 149/200] Loss 0.6627, train acc 0.6663\n",
      "Accuracy: 0.7240\n",
      "[Epoch 150/200] Loss 0.6885, train acc 0.6657\n",
      "[Epoch 151/200] Loss 0.6934, train acc 0.6487\n",
      "[Epoch 152/200] Loss 0.7030, train acc 0.6528\n",
      "[Epoch 153/200] Loss 0.6901, train acc 0.6575\n",
      "[Epoch 154/200] Loss 0.7111, train acc 0.6534\n",
      "[Epoch 155/200] Loss 0.6869, train acc 0.6563\n",
      "[Epoch 156/200] Loss 0.6752, train acc 0.6569\n",
      "[Epoch 157/200] Loss 0.6994, train acc 0.6587\n",
      "[Epoch 158/200] Loss 0.6612, train acc 0.6721\n",
      "[Epoch 159/200] Loss 0.6266, train acc 0.6915\n",
      "Accuracy: 0.7140\n",
      "[Epoch 160/200] Loss 0.6907, train acc 0.6499\n",
      "[Epoch 161/200] Loss 0.7110, train acc 0.6569\n",
      "[Epoch 162/200] Loss 0.6864, train acc 0.6645\n",
      "[Epoch 163/200] Loss 0.6737, train acc 0.6674\n",
      "[Epoch 164/200] Loss 0.6898, train acc 0.6575\n",
      "[Epoch 165/200] Loss 0.7010, train acc 0.6587\n",
      "[Epoch 166/200] Loss 0.6908, train acc 0.6569\n",
      "[Epoch 167/200] Loss 0.6652, train acc 0.6692\n",
      "[Epoch 168/200] Loss 0.7126, train acc 0.6546\n",
      "[Epoch 169/200] Loss 0.6823, train acc 0.6563\n",
      "Accuracy: 0.7200\n",
      "[Epoch 170/200] Loss 0.6744, train acc 0.6598\n",
      "[Epoch 171/200] Loss 0.7029, train acc 0.6552\n",
      "[Epoch 172/200] Loss 0.6778, train acc 0.6780\n",
      "[Epoch 173/200] Loss 0.7040, train acc 0.6493\n",
      "[Epoch 174/200] Loss 0.6801, train acc 0.6511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 175/200] Loss 0.7039, train acc 0.6364\n",
      "[Epoch 176/200] Loss 0.7229, train acc 0.6505\n",
      "[Epoch 177/200] Loss 0.6950, train acc 0.6522\n",
      "[Epoch 178/200] Loss 0.6876, train acc 0.6616\n",
      "[Epoch 179/200] Loss 0.6963, train acc 0.6499\n",
      "Accuracy: 0.7300\n",
      "[Epoch 180/200] Loss 0.6808, train acc 0.6674\n",
      "[Epoch 181/200] Loss 0.7255, train acc 0.6464\n",
      "[Epoch 182/200] Loss 0.6827, train acc 0.6663\n",
      "[Epoch 183/200] Loss 0.7208, train acc 0.6452\n",
      "[Epoch 184/200] Loss 0.6974, train acc 0.6423\n",
      "[Epoch 185/200] Loss 0.6904, train acc 0.6534\n",
      "[Epoch 186/200] Loss 0.6961, train acc 0.6434\n",
      "[Epoch 187/200] Loss 0.7071, train acc 0.6511\n",
      "[Epoch 188/200] Loss 0.6924, train acc 0.6358\n",
      "[Epoch 189/200] Loss 0.6977, train acc 0.6528\n",
      "Accuracy: 0.7140\n",
      "[Epoch 190/200] Loss 0.6926, train acc 0.6563\n",
      "[Epoch 191/200] Loss 0.6875, train acc 0.6651\n",
      "[Epoch 192/200] Loss 0.6760, train acc 0.6663\n",
      "[Epoch 193/200] Loss 0.6824, train acc 0.6563\n",
      "[Epoch 194/200] Loss 0.6579, train acc 0.6803\n",
      "[Epoch 195/200] Loss 0.7136, train acc 0.6470\n",
      "[Epoch 196/200] Loss 0.6904, train acc 0.6651\n",
      "[Epoch 197/200] Loss 0.6496, train acc 0.6704\n",
      "[Epoch 198/200] Loss 0.7299, train acc 0.6376\n",
      "[Epoch 199/200] Loss 0.6931, train acc 0.6557\n",
      "Accuracy: 0.7320\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_mask = torch.zeros(node_num, dtype=torch.uint8)\n",
    "train_mask[:node_num - 1000] = 1                  # 1700左右training\n",
    "val_mask = None                                    # 0valid\n",
    "test_mask = torch.zeros(node_num, dtype=torch.uint8)\n",
    "test_mask[node_num - 500:] = 1                    # 500test\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = OriLinearGNN(node_num, feat_dim, stat_dim, num_class, T).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "feat_Matrix = feat_Matrix.to(device)\n",
    "X_Node = X_Node.to(device)\n",
    "X_Neis = X_Neis.to(device)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get output\n",
    "    out = model(feat_Matrix, X_Node, X_Neis, dg_list)\n",
    "    \n",
    "    # Get loss\n",
    "    loss = F.nll_loss(out[train_mask], label_list[train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get predictions and calculate training accuracy\n",
    "    correct = float(pred[train_mask].eq(label_list[train_mask]).sum().item())\n",
    "    acc = correct / train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation on test data every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        _, pred = model(feat_Matrix, X_Node, X_Neis, dg_list).max(dim=1)\n",
    "        correct = float(pred[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "        acc = correct / test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_mask = torch.zeros(node_num, dtype=torch.uint8)\n",
    "train_mask[:node_num - 1000] = 1                  # 1700左右training\n",
    "val_mask = None                                    # 0valid\n",
    "test_mask = torch.zeros(node_num, dtype=torch.uint8)\n",
    "test_mask[node_num - 500:] = 1                    # 500test\n",
    "x = feat_Matrix\n",
    "edge_index = torch.from_numpy(cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggrSum(nn.Module):\n",
    "    def __init__(self, node_num):\n",
    "        super(AggrSum, self).__init__()\n",
    "        self.V = node_num\n",
    "    \n",
    "    def forward(self, H, X_node):\n",
    "        # H : (N, s) -> (V, s)\n",
    "        # X_node : (N, )\n",
    "        mask = torch.stack([X_node] * self.V, 0)\n",
    "        mask = mask.float() - torch.unsqueeze(torch.range(0,self.V-1).float(), 1)\n",
    "        mask = (mask == 0).float()\n",
    "        # (V, N) * (N, s) -> (V, s)\n",
    "        return torch.mm(mask, H)\n",
    "\n",
    "\n",
    "class GCNConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, node_num):\n",
    "        super(GCNConv, self).__init__()\n",
    "        self.linear = nn.Linear(in_channel, out_channel)\n",
    "        self.aggregation = AggrSum(node_num)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Add self-connect edges\n",
    "        edge_index = self.addSelfConnect(edge_index, x.shape[0])\n",
    "        \n",
    "        # Apply linear transform\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Normalize message\n",
    "        row, col = edge_index\n",
    "        deg = self.calDegree(row, x.shape[0]).float()\n",
    "        deg_sqrt = deg.pow(-0.5)  # (N, )\n",
    "        norm = deg_sqrt[row] * deg_sqrt[col]\n",
    "        \n",
    "        # Node feature matrix\n",
    "        tar_matrix = torch.index_select(x, dim=0, index=col)\n",
    "        tar_matrix = norm.view(-1, 1) * tar_matrix  # (E, out_channel)\n",
    "        # Aggregate information\n",
    "        aggr =  self.aggregation(tar_matrix, row)  # (N, out_channel)\n",
    "        return aggr\n",
    "        \n",
    "    \n",
    "    def calDegree(self, edges, num_nodes):\n",
    "        ind, deg = np.unique(edges.cpu().numpy(), return_counts=True)\n",
    "        deg_tensor = torch.zeros((num_nodes, ), dtype=torch.long)\n",
    "        deg_tensor[ind] = torch.from_numpy(deg)\n",
    "        return deg_tensor.to(edges.device)\n",
    "    \n",
    "    def addSelfConnect(self, edge_index, num_nodes):\n",
    "        selfconn = torch.stack([torch.range(0, num_nodes-1, dtype=torch.long)]*2,\n",
    "                               dim=0).to(edge_index.device)\n",
    "        return torch.cat(tensors=[edge_index, selfconn],\n",
    "                         dim=1)\n",
    "    \n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, feat_dim, num_class, num_node):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(feat_dim, 16, num_node)\n",
    "        self.conv2 = GCNConv(16, num_class, num_node)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivyone/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "/Users/ivyone/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] Loss 1.9893, train acc 0.0632\n",
      "[Epoch 1/200] Loss 1.9549, train acc 0.0989\n",
      "[Epoch 2/200] Loss 1.9022, train acc 0.2652\n",
      "[Epoch 3/200] Loss 1.8434, train acc 0.4344\n",
      "[Epoch 4/200] Loss 1.7754, train acc 0.4906\n",
      "[Epoch 5/200] Loss 1.7042, train acc 0.5047\n",
      "[Epoch 6/200] Loss 1.6479, train acc 0.4795\n",
      "[Epoch 7/200] Loss 1.5631, train acc 0.4783\n",
      "[Epoch 8/200] Loss 1.5176, train acc 0.4666\n",
      "[Epoch 9/200] Loss 1.4271, train acc 0.5293\n",
      "Accuracy: 0.4740\n",
      "[Epoch 10/200] Loss 1.3615, train acc 0.5398\n",
      "[Epoch 11/200] Loss 1.2943, train acc 0.5966\n",
      "[Epoch 12/200] Loss 1.2342, train acc 0.6587\n",
      "[Epoch 13/200] Loss 1.1590, train acc 0.6821\n",
      "[Epoch 14/200] Loss 1.1094, train acc 0.7272\n",
      "[Epoch 15/200] Loss 1.0289, train acc 0.7617\n",
      "[Epoch 16/200] Loss 0.9938, train acc 0.7740\n",
      "[Epoch 17/200] Loss 0.9277, train acc 0.7851\n",
      "[Epoch 18/200] Loss 0.9150, train acc 0.7881\n",
      "[Epoch 19/200] Loss 0.8312, train acc 0.8056\n",
      "Accuracy: 0.7520\n",
      "[Epoch 20/200] Loss 0.7828, train acc 0.7963\n",
      "[Epoch 21/200] Loss 0.7430, train acc 0.8144\n",
      "[Epoch 22/200] Loss 0.7212, train acc 0.8249\n",
      "[Epoch 23/200] Loss 0.7001, train acc 0.8331\n",
      "[Epoch 24/200] Loss 0.6233, train acc 0.8536\n",
      "[Epoch 25/200] Loss 0.6170, train acc 0.8413\n",
      "[Epoch 26/200] Loss 0.5767, train acc 0.8560\n",
      "[Epoch 27/200] Loss 0.5564, train acc 0.8536\n",
      "[Epoch 28/200] Loss 0.5596, train acc 0.8507\n",
      "[Epoch 29/200] Loss 0.5112, train acc 0.8811\n",
      "Accuracy: 0.8360\n",
      "[Epoch 30/200] Loss 0.5109, train acc 0.8724\n",
      "[Epoch 31/200] Loss 0.4771, train acc 0.8817\n",
      "[Epoch 32/200] Loss 0.4600, train acc 0.8823\n",
      "[Epoch 33/200] Loss 0.4506, train acc 0.8730\n",
      "[Epoch 34/200] Loss 0.4470, train acc 0.8741\n",
      "[Epoch 35/200] Loss 0.4425, train acc 0.8806\n",
      "[Epoch 36/200] Loss 0.4106, train acc 0.8876\n",
      "[Epoch 37/200] Loss 0.4188, train acc 0.8788\n",
      "[Epoch 38/200] Loss 0.3674, train acc 0.9011\n",
      "[Epoch 39/200] Loss 0.3644, train acc 0.8911\n",
      "Accuracy: 0.8400\n",
      "[Epoch 40/200] Loss 0.3575, train acc 0.9022\n",
      "[Epoch 41/200] Loss 0.3660, train acc 0.9040\n",
      "[Epoch 42/200] Loss 0.3599, train acc 0.8952\n",
      "[Epoch 43/200] Loss 0.3442, train acc 0.9063\n",
      "[Epoch 44/200] Loss 0.3312, train acc 0.9005\n",
      "[Epoch 45/200] Loss 0.3374, train acc 0.9075\n",
      "[Epoch 46/200] Loss 0.3066, train acc 0.9063\n",
      "[Epoch 47/200] Loss 0.3095, train acc 0.9069\n",
      "[Epoch 48/200] Loss 0.3340, train acc 0.9022\n",
      "[Epoch 49/200] Loss 0.3148, train acc 0.9011\n",
      "Accuracy: 0.8440\n",
      "[Epoch 50/200] Loss 0.3038, train acc 0.9180\n",
      "[Epoch 51/200] Loss 0.3057, train acc 0.9116\n",
      "[Epoch 52/200] Loss 0.2967, train acc 0.9128\n",
      "[Epoch 53/200] Loss 0.2897, train acc 0.9169\n",
      "[Epoch 54/200] Loss 0.2842, train acc 0.9227\n",
      "[Epoch 55/200] Loss 0.2774, train acc 0.9133\n",
      "[Epoch 56/200] Loss 0.2581, train acc 0.9204\n",
      "[Epoch 57/200] Loss 0.2781, train acc 0.9204\n",
      "[Epoch 58/200] Loss 0.2869, train acc 0.9215\n",
      "[Epoch 59/200] Loss 0.2982, train acc 0.9087\n",
      "Accuracy: 0.8380\n",
      "[Epoch 60/200] Loss 0.2733, train acc 0.9221\n",
      "[Epoch 61/200] Loss 0.2671, train acc 0.9198\n",
      "[Epoch 62/200] Loss 0.2678, train acc 0.9221\n",
      "[Epoch 63/200] Loss 0.2658, train acc 0.9221\n",
      "[Epoch 64/200] Loss 0.2670, train acc 0.9186\n",
      "[Epoch 65/200] Loss 0.2583, train acc 0.9186\n",
      "[Epoch 66/200] Loss 0.2645, train acc 0.9174\n",
      "[Epoch 67/200] Loss 0.2597, train acc 0.9186\n",
      "[Epoch 68/200] Loss 0.2665, train acc 0.9204\n",
      "[Epoch 69/200] Loss 0.2592, train acc 0.9245\n",
      "Accuracy: 0.8360\n",
      "[Epoch 70/200] Loss 0.2559, train acc 0.9221\n",
      "[Epoch 71/200] Loss 0.2607, train acc 0.9280\n",
      "[Epoch 72/200] Loss 0.2644, train acc 0.9262\n",
      "[Epoch 73/200] Loss 0.2504, train acc 0.9233\n",
      "[Epoch 74/200] Loss 0.2494, train acc 0.9239\n",
      "[Epoch 75/200] Loss 0.2540, train acc 0.9315\n",
      "[Epoch 76/200] Loss 0.2507, train acc 0.9292\n",
      "[Epoch 77/200] Loss 0.2445, train acc 0.9286\n",
      "[Epoch 78/200] Loss 0.2369, train acc 0.9198\n",
      "[Epoch 79/200] Loss 0.2363, train acc 0.9303\n",
      "Accuracy: 0.8300\n",
      "[Epoch 80/200] Loss 0.2337, train acc 0.9280\n",
      "[Epoch 81/200] Loss 0.2383, train acc 0.9262\n",
      "[Epoch 82/200] Loss 0.2422, train acc 0.9315\n",
      "[Epoch 83/200] Loss 0.2403, train acc 0.9333\n",
      "[Epoch 84/200] Loss 0.2295, train acc 0.9245\n",
      "[Epoch 85/200] Loss 0.2611, train acc 0.9292\n",
      "[Epoch 86/200] Loss 0.2347, train acc 0.9297\n",
      "[Epoch 87/200] Loss 0.2315, train acc 0.9327\n",
      "[Epoch 88/200] Loss 0.2293, train acc 0.9315\n",
      "[Epoch 89/200] Loss 0.2339, train acc 0.9309\n",
      "Accuracy: 0.8340\n",
      "[Epoch 90/200] Loss 0.2335, train acc 0.9280\n",
      "[Epoch 91/200] Loss 0.2170, train acc 0.9368\n",
      "[Epoch 92/200] Loss 0.2304, train acc 0.9303\n",
      "[Epoch 93/200] Loss 0.2332, train acc 0.9309\n",
      "[Epoch 94/200] Loss 0.2215, train acc 0.9362\n",
      "[Epoch 95/200] Loss 0.2337, train acc 0.9356\n",
      "[Epoch 96/200] Loss 0.2220, train acc 0.9333\n",
      "[Epoch 97/200] Loss 0.2346, train acc 0.9274\n",
      "[Epoch 98/200] Loss 0.2229, train acc 0.9350\n",
      "[Epoch 99/200] Loss 0.2286, train acc 0.9280\n",
      "Accuracy: 0.8260\n",
      "[Epoch 100/200] Loss 0.2269, train acc 0.9374\n",
      "[Epoch 101/200] Loss 0.2156, train acc 0.9338\n",
      "[Epoch 102/200] Loss 0.2286, train acc 0.9327\n",
      "[Epoch 103/200] Loss 0.2190, train acc 0.9432\n",
      "[Epoch 104/200] Loss 0.2101, train acc 0.9350\n",
      "[Epoch 105/200] Loss 0.2113, train acc 0.9397\n",
      "[Epoch 106/200] Loss 0.2142, train acc 0.9333\n",
      "[Epoch 107/200] Loss 0.2024, train acc 0.9391\n",
      "[Epoch 108/200] Loss 0.2206, train acc 0.9338\n",
      "[Epoch 109/200] Loss 0.2044, train acc 0.9356\n",
      "Accuracy: 0.8240\n",
      "[Epoch 110/200] Loss 0.2278, train acc 0.9338\n",
      "[Epoch 111/200] Loss 0.2160, train acc 0.9321\n",
      "[Epoch 112/200] Loss 0.2035, train acc 0.9379\n",
      "[Epoch 113/200] Loss 0.2103, train acc 0.9391\n",
      "[Epoch 114/200] Loss 0.2199, train acc 0.9379\n",
      "[Epoch 115/200] Loss 0.2066, train acc 0.9391\n",
      "[Epoch 116/200] Loss 0.2149, train acc 0.9268\n",
      "[Epoch 117/200] Loss 0.2165, train acc 0.9292\n",
      "[Epoch 118/200] Loss 0.2142, train acc 0.9327\n",
      "[Epoch 119/200] Loss 0.2026, train acc 0.9379\n",
      "Accuracy: 0.8180\n",
      "[Epoch 120/200] Loss 0.1992, train acc 0.9432\n",
      "[Epoch 121/200] Loss 0.2024, train acc 0.9467\n",
      "[Epoch 122/200] Loss 0.1962, train acc 0.9415\n",
      "[Epoch 123/200] Loss 0.1948, train acc 0.9368\n",
      "[Epoch 124/200] Loss 0.2113, train acc 0.9274\n",
      "[Epoch 125/200] Loss 0.1927, train acc 0.9385\n",
      "[Epoch 126/200] Loss 0.2058, train acc 0.9397\n",
      "[Epoch 127/200] Loss 0.1976, train acc 0.9409\n",
      "[Epoch 128/200] Loss 0.1894, train acc 0.9420\n",
      "[Epoch 129/200] Loss 0.1908, train acc 0.9368\n",
      "Accuracy: 0.8200\n",
      "[Epoch 130/200] Loss 0.2049, train acc 0.9344\n",
      "[Epoch 131/200] Loss 0.2087, train acc 0.9344\n",
      "[Epoch 132/200] Loss 0.1984, train acc 0.9450\n",
      "[Epoch 133/200] Loss 0.1969, train acc 0.9432\n",
      "[Epoch 134/200] Loss 0.1971, train acc 0.9438\n",
      "[Epoch 135/200] Loss 0.1977, train acc 0.9338\n",
      "[Epoch 136/200] Loss 0.2014, train acc 0.9426\n",
      "[Epoch 137/200] Loss 0.1919, train acc 0.9450\n",
      "[Epoch 138/200] Loss 0.1899, train acc 0.9432\n",
      "[Epoch 139/200] Loss 0.1921, train acc 0.9385\n",
      "Accuracy: 0.8140\n",
      "[Epoch 140/200] Loss 0.1953, train acc 0.9397\n",
      "[Epoch 141/200] Loss 0.1921, train acc 0.9426\n",
      "[Epoch 142/200] Loss 0.1924, train acc 0.9438\n",
      "[Epoch 143/200] Loss 0.1976, train acc 0.9456\n",
      "[Epoch 144/200] Loss 0.1943, train acc 0.9397\n",
      "[Epoch 145/200] Loss 0.2018, train acc 0.9420\n",
      "[Epoch 146/200] Loss 0.1968, train acc 0.9374\n",
      "[Epoch 147/200] Loss 0.1991, train acc 0.9379\n",
      "[Epoch 148/200] Loss 0.1950, train acc 0.9409\n",
      "[Epoch 149/200] Loss 0.1749, train acc 0.9467\n",
      "Accuracy: 0.8180\n",
      "[Epoch 150/200] Loss 0.1996, train acc 0.9315\n",
      "[Epoch 151/200] Loss 0.1829, train acc 0.9461\n",
      "[Epoch 152/200] Loss 0.1808, train acc 0.9444\n",
      "[Epoch 153/200] Loss 0.1911, train acc 0.9409\n",
      "[Epoch 154/200] Loss 0.1815, train acc 0.9385\n",
      "[Epoch 155/200] Loss 0.1769, train acc 0.9415\n",
      "[Epoch 156/200] Loss 0.1942, train acc 0.9368\n",
      "[Epoch 157/200] Loss 0.1929, train acc 0.9403\n",
      "[Epoch 158/200] Loss 0.1838, train acc 0.9397\n",
      "[Epoch 159/200] Loss 0.1859, train acc 0.9467\n",
      "Accuracy: 0.8180\n",
      "[Epoch 160/200] Loss 0.1863, train acc 0.9438\n",
      "[Epoch 161/200] Loss 0.1848, train acc 0.9473\n",
      "[Epoch 162/200] Loss 0.1793, train acc 0.9514\n",
      "[Epoch 163/200] Loss 0.1984, train acc 0.9391\n",
      "[Epoch 164/200] Loss 0.1931, train acc 0.9444\n",
      "[Epoch 165/200] Loss 0.1962, train acc 0.9409\n",
      "[Epoch 166/200] Loss 0.1801, train acc 0.9368\n",
      "[Epoch 167/200] Loss 0.1992, train acc 0.9432\n",
      "[Epoch 168/200] Loss 0.1713, train acc 0.9438\n",
      "[Epoch 169/200] Loss 0.1944, train acc 0.9391\n",
      "Accuracy: 0.8180\n",
      "[Epoch 170/200] Loss 0.1923, train acc 0.9444\n",
      "[Epoch 171/200] Loss 0.1866, train acc 0.9438\n",
      "[Epoch 172/200] Loss 0.1914, train acc 0.9415\n",
      "[Epoch 173/200] Loss 0.1775, train acc 0.9444\n",
      "[Epoch 174/200] Loss 0.1828, train acc 0.9456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 175/200] Loss 0.1725, train acc 0.9456\n",
      "[Epoch 176/200] Loss 0.1713, train acc 0.9520\n",
      "[Epoch 177/200] Loss 0.1936, train acc 0.9385\n",
      "[Epoch 178/200] Loss 0.1603, train acc 0.9555\n",
      "[Epoch 179/200] Loss 0.1790, train acc 0.9479\n",
      "Accuracy: 0.8200\n",
      "[Epoch 180/200] Loss 0.1785, train acc 0.9485\n",
      "[Epoch 181/200] Loss 0.1751, train acc 0.9479\n",
      "[Epoch 182/200] Loss 0.1817, train acc 0.9467\n",
      "[Epoch 183/200] Loss 0.1705, train acc 0.9502\n",
      "[Epoch 184/200] Loss 0.1683, train acc 0.9467\n",
      "[Epoch 185/200] Loss 0.1693, train acc 0.9467\n",
      "[Epoch 186/200] Loss 0.1839, train acc 0.9444\n",
      "[Epoch 187/200] Loss 0.1679, train acc 0.9532\n",
      "[Epoch 188/200] Loss 0.1691, train acc 0.9496\n",
      "[Epoch 189/200] Loss 0.1734, train acc 0.9461\n",
      "Accuracy: 0.8200\n",
      "[Epoch 190/200] Loss 0.1849, train acc 0.9397\n",
      "[Epoch 191/200] Loss 0.1757, train acc 0.9432\n",
      "[Epoch 192/200] Loss 0.1758, train acc 0.9444\n",
      "[Epoch 193/200] Loss 0.1787, train acc 0.9461\n",
      "[Epoch 194/200] Loss 0.1695, train acc 0.9432\n",
      "[Epoch 195/200] Loss 0.1685, train acc 0.9514\n",
      "[Epoch 196/200] Loss 0.1813, train acc 0.9444\n",
      "[Epoch 197/200] Loss 0.1728, train acc 0.9467\n",
      "[Epoch 198/200] Loss 0.1684, train acc 0.9573\n",
      "[Epoch 199/200] Loss 0.1681, train acc 0.9514\n",
      "Accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(feat_dim, num_class, node_num).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "x = x.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get output\n",
    "    out = model(x, edge_index)\n",
    "    \n",
    "    # Get loss\n",
    "    loss = F.nll_loss(out[train_mask], label_list[train_mask])\n",
    "    _, pred = out.max(dim=1)\n",
    "    \n",
    "    # Get predictions and calculate training accuracy\n",
    "    correct = float(pred[train_mask].eq(label_list[train_mask]).sum().item())\n",
    "    acc = correct / train_mask.sum().item()\n",
    "    print('[Epoch {}/200] Loss {:.4f}, train acc {:.4f}'.format(epoch, loss.cpu().detach().data.item(), acc))\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation on test data every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        _, pred = model(x, edge_index).max(dim=1)\n",
    "        correct = float(pred[test_mask].eq(label_list[test_mask]).sum().item())\n",
    "        acc = correct / test_mask.sum().item()\n",
    "        print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
